```
# ========== ML PLATFORM SERVICE (Python/FastAPI) ==========

# ml-platform/src/main.py
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import uvicorn
import logging
from typing import Optional, List, Dict, Any
import asyncio

from app.config import settings
from app.models import db
from app.routers import (
    model_router,
    training_router,
    inference_router,
    dataset_router
)
from app.services import ModelRegistry, TrainingService, InferenceService
from app.kafka import kafka_consumer, kafka_producer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Starting ML Platform Service...")
    
    # Initialize database
    await db.init_db()
    
    # Start Kafka consumer
    asyncio.create_task(kafka_consumer.start())
    
    # Register with service discovery
    await register_service()
    
    yield
    
    # Shutdown
    logger.info("Shutting down ML Platform Service...")
    await kafka_consumer.stop()
    await db.close()

app = FastAPI(
    title="AI Speaker ML Platform",
    version="1.0.0",
    lifespan=lifespan
)

# CORS 설정
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 라우터 등록
app.include_router(model_router, prefix="/api/v1/models", tags=["models"])
app.include_router(training_router, prefix="/api/v1/training", tags=["training"])
app.include_router(inference_router, prefix="/api/v1/inference", tags=["inference"])
app.include_router(dataset_router, prefix="/api/v1/datasets", tags=["datasets"])

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "ml-platform"}

# ml-platform/src/app/models/ml_models.py
from sqlalchemy import Column, String, JSON, DateTime, Float, Integer, Boolean, ForeignKey
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime
import uuid

Base = declarative_base()

class MLModel(Base):
    __tablename__ = "ml_models"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    name = Column(String, nullable=False)
    version = Column(String, nullable=False)
    model_type = Column(String, nullable=False)  # nlu, tts, stt, personalization
    framework = Column(String)  # tensorflow, pytorch, etc
    
    description = Column(String)
    metadata = Column(JSON)
    
    # 모델 파일 정보
    model_path = Column(String)
    model_size = Column(Integer)
    
    # 성능 메트릭
    metrics = Column(JSON)
    accuracy = Column(Float)
    latency_ms = Column(Float)
    
    # 상태
    status = Column(String, default="draft")  # draft, training, ready, deployed, archived
    is_active = Column(Boolean, default=False)
    
    # 배포 정보
    deployment_config = Column(JSON)
    endpoint_url = Column(String)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    created_by = Column(String)
    
    # 관계
    training_jobs = relationship("TrainingJob", back_populates="model")
    versions = relationship("ModelVersion", back_populates="model")

class TrainingJob(Base):
    __tablename__ = "training_jobs"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    model_id = Column(String, ForeignKey("ml_models.id"))
    
    job_type = Column(String)  # train, fine-tune, evaluate
    status = Column(String, default="pending")  # pending, running, completed, failed
    
    # 학습 설정
    training_config = Column(JSON)
    hyperparameters = Column(JSON)
    dataset_id = Column(String)
    
    # 리소스 할당
    resources = Column(JSON)  # GPU, memory, etc
    
    # 진행 상황
    progress = Column(Float, default=0.0)
    current_epoch = Column(Integer, default=0)
    total_epochs = Column(Integer)
    
    # 결과
    training_metrics = Column(JSON)
    validation_metrics = Column(JSON)
    best_checkpoint = Column(String)
    
    # 로그
    log_path = Column(String)
    tensorboard_path = Column(String)
    
    started_at = Column(DateTime)
    completed_at = Column(DateTime)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # 관계
    model = relationship("MLModel", back_populates="training_jobs")

class Dataset(Base):
    __tablename__ = "datasets"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    name = Column(String, nullable=False)
    dataset_type = Column(String)  # conversation, audio, sensor_data
    
    description = Column(String)
    metadata = Column(JSON)
    
    # 데이터 정보
    data_path = Column(String)
    size_mb = Column(Float)
    sample_count = Column(Integer)
    
    # 데이터 스키마
    schema = Column(JSON)
    features = Column(JSON)
    labels = Column(JSON)
    
    # 전처리 정보
    preprocessing_config = Column(JSON)
    is_preprocessed = Column(Boolean, default=False)
    
    # 분할 정보
    train_split = Column(Float, default=0.8)
    val_split = Column(Float, default=0.1)
    test_split = Column(Float, default=0.1)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    created_by = Column(String)

# ml-platform/src/app/services/model_registry.py
import logging
from typing import List, Optional, Dict, Any
from datetime import datetime
import mlflow
import mlflow.tensorflow
import mlflow.pytorch
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

from app.models.ml_models import MLModel, ModelVersion
from app.storage import model_storage
from app.config import settings

logger = logging.getLogger(__name__)

class ModelRegistry:
    def __init__(self):
        mlflow.set_tracking_uri(settings.MLFLOW_TRACKING_URI)
        self.storage = model_storage
        
    async def register_model(
        self,
        session: AsyncSession,
        model_data: Dict[str, Any],
        model_file: bytes
    ) -> MLModel:
        """새 모델 등록"""
        try:
            # 모델 메타데이터 생성
            model = MLModel(
                name=model_data["name"],
                version=model_data["version"],
                model_type=model_data["model_type"],
                framework=model_data.get("framework", "tensorflow"),
                description=model_data.get("description"),
                metadata=model_data.get("metadata", {}),
                created_by=model_data["created_by"]
            )
            
            # 모델 파일 저장
            model_path = await self.storage.save_model(
                model.id,
                model.version,
                model_file
            )
            model.model_path = model_path
            model.model_size = len(model_file)
            
            # MLflow에 등록
            with mlflow.start_run():
                if model.framework == "tensorflow":
                    mlflow.tensorflow.log_model(
                        tf_saved_model_dir=model_path,
                        artifact_path="model",
                        registered_model_name=model.name
                    )
                elif model.framework == "pytorch":
                    mlflow.pytorch.log_model(
                        pytorch_model=model_path,
                        artifact_path="model",
                        registered_model_name=model.name
                    )
                
                # 메트릭 로깅
                if model_data.get("metrics"):
                    for key, value in model_data["metrics"].items():
                        mlflow.log_metric(key, value)
            
            # DB 저장
            session.add(model)
            await session.commit()
            
            logger.info(f"Model registered: {model.name} v{model.version}")
            return model
            
        except Exception as e:
            logger.error(f"Failed to register model: {e}")
            await session.rollback()
            raise
    
    async def get_model(
        self,
        session: AsyncSession,
        model_id: str
    ) -> Optional[MLModel]:
        """모델 조회"""
        result = await session.execute(
            select(MLModel).where(MLModel.id == model_id)
        )
        return result.scalar_one_or_none()
    
    async def list_models(
        self,
        session: AsyncSession,
        model_type: Optional[str] = None,
        status: Optional[str] = None
    ) -> List[MLModel]:
        """모델 목록 조회"""
        query = select(MLModel)
        
        if model_type:
            query = query.where(MLModel.model_type == model_type)
        if status:
            query = query.where(MLModel.status == status)
            
        result = await session.execute(query)
        return result.scalars().all()
    
    async def deploy_model(
        self,
        session: AsyncSession,
        model_id: str,
        deployment_config: Dict[str, Any]
    ) -> MLModel:
        """모델 배포"""
        model = await self.get_model(session, model_id)
        if not model:
            raise ValueError(f"Model not found: {model_id}")
        
        # 배포 설정
        model.deployment_config = deployment_config
        model.status = "deployed"
        model.is_active = True
        
        # 엔드포인트 생성
        endpoint_url = await self._create_model_endpoint(model, deployment_config)
        model.endpoint_url = endpoint_url
        
        await session.commit()
        
        logger.info(f"Model deployed: {model.name} at {endpoint_url}")
        return model
    
    async def _create_model_endpoint(
        self,
        model: MLModel,
        config: Dict[str, Any]
    ) -> str:
        """모델 엔드포인트 생성"""
        # Kubernetes에 모델 서빙 파드 배포
        # TensorFlow Serving 또는 TorchServe 사용
        # 실제 구현은 K8s API 호출
        endpoint_url = f"http://model-serving/{model.id}/predict"
        return endpoint_url

# ml-platform/src/app/services/training_service.py
import logging
from typing import Dict, Any, Optional
import asyncio
from datetime import datetime
from celery import Celery
import torch
import tensorflow as tf
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.ml_models import TrainingJob, MLModel, Dataset
from app.config import settings
from app.kafka import kafka_producer

logger = logging.getLogger(__name__)

# Celery 설정
celery_app = Celery(
    'training',
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND
)

class TrainingService:
    def __init__(self):
        self.active_jobs = {}
        
    async def create_training_job(
        self,
        session: AsyncSession,
        job_data: Dict[str, Any]
    ) -> TrainingJob:
        """학습 작업 생성"""
        job = TrainingJob(
            model_id=job_data["model_id"],
            job_type=job_data["job_type"],
            training_config=job_data["training_config"],
            hyperparameters=job_data["hyperparameters"],
            dataset_id=job_data["dataset_id"],
            resources=job_data.get("resources", {
                "gpu": 1,
                "memory": "16Gi",
                "cpu": 4
            })
        )
        
        session.add(job)
        await session.commit()
        
        # Celery 태스크 실행
        task = train_model_task.delay(job.id)
        self.active_jobs[job.id] = task.id
        
        # 이벤트 발행
        await kafka_producer.send(
            "training-events",
            {
                "type": "training.started",
                "job_id": job.id,
                "model_id": job.model_id,
                "timestamp": datetime.utcnow().isoformat()
            }
        )
        
        return job
    
    async def get_job_status(
        self,
        session: AsyncSession,
        job_id: str
    ) -> Dict[str, Any]:
        """학습 작업 상태 조회"""
        job = await session.get(TrainingJob, job_id)
        if not job:
            raise ValueError(f"Job not found: {job_id}")
        
        # Celery 태스크 상태 확인
        if job.id in self.active_jobs:
            task_id = self.active_jobs[job.id]
            task_result = celery_app.AsyncResult(task_id)
            
            return {
                "job_id": job.id,
                "status": job.status,
                "progress": job.progress,
                "current_epoch": job.current_epoch,
                "total_epochs": job.total_epochs,
                "metrics": job.training_metrics,
                "task_status": task_result.status,
                "eta": self._calculate_eta(job)
            }
        
        return {
            "job_id": job.id,
            "status": job.status,
            "progress": job.progress,
            "metrics": job.training_metrics
        }
    
    async def stop_training(
        self,
        session: AsyncSession,
        job_id: str
    ) -> TrainingJob:
        """학습 중단"""
        job = await session.get(TrainingJob, job_id)
        if not job:
            raise ValueError(f"Job not found: {job_id}")
        
        if job.id in self.active_jobs:
            task_id = self.active_jobs[job.id]
            celery_app.control.revoke(task_id, terminate=True)
            
        job.status = "stopped"
        await session.commit()
        
        return job

# Celery 태스크
@celery_app.task(bind=True)
def train_model_task(self, job_id: str):
    """모델 학습 태스크"""
    logger.info(f"Starting training job: {job_id}")
    
    try:
        # 동기 DB 세션 사용
        from app.models import get_sync_session
        
        with get_sync_session() as session:
            job = session.query(TrainingJob).filter_by(id=job_id).first()
            if not job:
                raise ValueError(f"Job not found: {job_id}")
            
            model = session.query(MLModel).filter_by(id=job.model_id).first()
            dataset = session.query(Dataset).filter_by(id=job.dataset_id).first()
            
            # 학습 시작
            job.status = "running"
            job.started_at = datetime.utcnow()
            session.commit()
            
            # 모델 타입에 따른 학습 실행
            if model.model_type == "nlu":
                train_nlu_model(job, model, dataset, session)
            elif model.model_type == "personalization":
                train_personalization_model(job, model, dataset, session)
            elif model.model_type == "stt":
                train_stt_model(job, model, dataset, session)
            else:
                raise ValueError(f"Unknown model type: {model.model_type}")
            
            # 학습 완료
            job.status = "completed"
            job.completed_at = datetime.utcnow()
            job.progress = 100.0
            session.commit()
            
            logger.info(f"Training completed for job: {job_id}")
            
    except Exception as e:
        logger.error(f"Training failed for job {job_id}: {e}")
        
        with get_sync_session() as session:
            job = session.query(TrainingJob).filter_by(id=job_id).first()
            if job:
                job.status = "failed"
                job.completed_at = datetime.utcnow()
                session.commit()
        
        raise

def train_nlu_model(job, model, dataset, session):
    """NLU 모델 학습"""
    import transformers
    from transformers import AutoModelForSequenceClassification, AutoTokenizer
    
    # 하이퍼파라미터
    batch_size = job.hyperparameters.get("batch_size", 32)
    learning_rate = job.hyperparameters.get("learning_rate", 2e-5)
    num_epochs = job.hyperparameters.get("num_epochs", 3)
    
    job.total_epochs = num_epochs
    
    # 데이터 로드
    train_data = load_dataset(dataset.data_path, "train")
    val_data = load_dataset(dataset.data_path, "validation")
    
    # 모델 초기화
    tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
    model = AutoModelForSequenceClassification.from_pretrained(
        "klue/bert-base",
        num_labels=len(dataset.labels)
    )
    
    # 학습 루프
    for epoch in range(num_epochs):
        job.current_epoch = epoch + 1
        
        # 에폭 학습
        train_metrics = train_epoch(model, train_data, batch_size, learning_rate)
        val_metrics = validate_epoch(model, val_data, batch_size)
        
        # 진행률 업데이트
        job.progress = (epoch + 1) / num_epochs * 100
        job.training_metrics = train_metrics
        job.validation_metrics = val_metrics
        
        session.commit()
        
        # 체크포인트 저장
        if val_metrics["accuracy"] > best_accuracy:
            checkpoint_path = save_checkpoint(model, epoch)
            job.best_checkpoint = checkpoint_path

# ml-platform/src/app/services/inference_service.py
import logging
from typing import Dict, Any, List, Optional
import asyncio
import aioredis
import numpy as np
from datetime import datetime

from app.models.ml_models import MLModel
from app.config import settings

logger = logging.getLogger(__name__)

class InferenceService:
    def __init__(self):
        self.loaded_models = {}
        self.model_cache = None
        self.request_queue = asyncio.Queue()
        
    async def initialize(self):
        """추론 서비스 초기화"""
        # Redis 캐시 연결
        self.model_cache = await aioredis.create_redis_pool(
            settings.REDIS_URL,
            encoding='utf-8'
        )
        
        # 자주 사용하는 모델 사전 로드
        await self._preload_models()
        
        # 배치 처리 워커 시작
        asyncio.create_task(self._batch_processing_worker())
    
    async def predict(
        self,
        model_id: str,
        input_data: Dict[str, Any],
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """단일 예측 수행"""
        try:
            # 모델 로드
            model = await self._get_or_load_model(model_id)
            
            # 입력 전처리
            processed_input = await self._preprocess_input(model, input_data)
            
            # 캐시 확인
            cache_key = self._generate_cache_key(model_id, processed_input)
            cached_result = await self.model_cache.get(cache_key)
            
            if cached_result:
                logger.debug(f"Cache hit for model {model_id}")
                return json.loads(cached_result)
            
            # 추론 실행
            start_time = datetime.utcnow()
            
            if options and options.get("batch_mode"):
                # 배치 큐에 추가
                future = asyncio.Future()
                await self.request_queue.put({
                    "model_id": model_id,
                    "input": processed_input,
                    "future": future
                })
                result = await future
            else:
                # 즉시 실행
                result = await self._run_inference(model, processed_input)
            
            end_time = datetime.utcnow()
            latency_ms = (end_time - start_time).total_seconds() * 1000
            
            # 결과 후처리
            output = await self._postprocess_output(model, result)
            
            response = {
                "model_id": model_id,
                "prediction": output,
                "latency_ms": latency_ms,
                "timestamp": end_time.isoformat()
            }
            
            # 캐시 저장 (TTL: 1시간)
            await self.model_cache.setex(
                cache_key,
                3600,
                json.dumps(response)
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Inference error for model {model_id}: {e}")
            raise
    
    async def batch_predict(
        self,
        model_id: str,
        input_batch: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """배치 예측 수행"""
        model = await self._get_or_load_model(model_id)
        
        # 입력 배치 전처리
        processed_batch = [
            await self._preprocess_input(model, input_data)
            for input_data in input_batch
        ]
        
        # 배치 추론
        results = await self._run_batch_inference(model, processed_batch)
        
        # 결과 후처리
        outputs = [
            await self._postprocess_output(model, result)
            for result in results
        ]
        
        return [
            {
                "model_id": model_id,
                "prediction": output,
                "timestamp": datetime.utcnow().isoformat()
            }
            for output in outputs
        ]
    
    async def _get_or_load_model(self, model_id: str):
        """모델 로드 (캐시 확인)"""
        if model_id in self.loaded_models:
            return self.loaded_models[model_id]
        
        # DB에서 모델 정보 조회
        async with get_session() as session:
            model = await session.get(MLModel, model_id)
            if not model or model.status != "deployed":
                raise ValueError(f"Model not available: {model_id}")
        
        # 모델 로드
        if model.framework == "tensorflow":
            loaded_model = await self._load_tensorflow_model(model)
        elif model.framework == "pytorch":
            loaded_model = await self._load_pytorch_model(model)
        else:
            raise ValueError(f"Unsupported framework: {model.framework}")
        
        self.loaded_models[model_id] = {
            "model": loaded_model,
            "metadata": model,
            "loaded_at": datetime.utcnow()
        }
        
        return self.loaded_models[model_id]
    
    async def _run_inference(self, model_info: Dict, input_data: Any) -> Any:
        """추론 실행"""
        model = model_info["model"]
        metadata = model_info["metadata"]
        
        if metadata.framework == "tensorflow":
            # TensorFlow 추론
            import tensorflow as tf
            return model(input_data)
        
        elif metadata.framework == "pytorch":
            # PyTorch 추론
            import torch
            with torch.no_grad():
                return model(torch.tensor(input_data))
        
    async def _batch_processing_worker(self):
        """배치 처리 워커"""
        while True:
            try:
                # 요청 수집 (최대 100ms 대기)
                batch = []
                deadline = asyncio.get_event_loop().time() + 0.1
                
                while len(batch) < 32:  # 최대 배치 크기
                    timeout = max(0, deadline - asyncio.get_event_loop().time())
                    
                    try:
                        request = await asyncio.wait_for(
                            self.request_queue.get(),
                            timeout=timeout
                        )
                        batch.append(request)
                    except asyncio.TimeoutError:
                        break
                
                if batch:
                    # 모델별로 그룹화
                    model_batches = {}
                    for req in batch:
                        model_id = req["model_id"]
                        if model_id not in model_batches:
                            model_batches[model_id] = []
                        model_batches[model_id].append(req)
                    
                    # 각 모델별로 배치 처리
                    for model_id, requests in model_batches.items():
                        model = await self._get_or_load_model(model_id)
                        inputs = [req["input"] for req in requests]
                        
                        # 배치 추론
                        results = await self._run_batch_inference(model, inputs)
                        
                        # 결과 반환
                        for req, result in zip(requests, results):
                            req["future"].set_result(result)
                
            except Exception as e:
                logger.error(f"Batch processing error: {e}")
                # 에러 시 모든 요청에 에러 전파
                for req in batch:
                    if not req["future"].done():
                        req["future"].set_exception(e)

# ml-platform/src/app/routers/inference.py
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from typing import List, Dict, Any
import logging

from app.services import InferenceService
from app.schemas import PredictionRequest, PredictionResponse, BatchPredictionRequest
from app.auth import get_current_user

logger = logging.getLogger(__name__)
router = APIRouter()

inference_service = InferenceService()

@router.post("/{model_id}/predict", response_model=PredictionResponse)
async def predict(
    model_id: str,
    request: PredictionRequest,
    background_tasks: BackgroundTasks,
    current_user: dict = Depends(get_current_user)
):
    """단일 예측 엔드포인트"""
    try:
        result = await inference_service.predict(
            model_id=model_id,
            input_data=request.input_data,
            options=request.options
        )
        
        # 사용량 추적 (백그라운드)
        background_tasks.add_task(
            track_usage,
            user_id=current_user["id"],
            model_id=model_id,
            request_type="single"
        )
        
        return PredictionResponse(**result)
        
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.post("/{model_id}/batch-predict", response_model=List[PredictionResponse])
async def batch_predict(
    model_id: str,
    request: BatchPredictionRequest,
    background_tasks: BackgroundTasks,
    current_user: dict = Depends(get_current_user)
):
    """배치 예측 엔드포인트"""
    try:
        results = await inference_service.batch_predict(
            model_id=model_id,
            input_batch=request.input_batch
        )
        
        # 사용량 추적 (백그라운드)
        background_tasks.add_task(
            track_usage,
            user_id=current_user["id"],
            model_id=model_id,
            request_type="batch",
            batch_size=len(request.input_batch)
        )
        
        return [PredictionResponse(**result) for result in results]
        
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Batch prediction error: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

async def track_usage(
    user_id: str,
    model_id: str,
    request_type: str,
    batch_size: int = 1
):
    """사용량 추적"""
    # Kafka로 사용량 이벤트 전송
    await kafka_producer.send(
        "ml-usage-events",
        {
            "user_id": user_id,
            "model_id": model_id,
            "request_type": request_type,
            "batch_size": batch_size,
            "timestamp": datetime.utcnow().isoformat()
        }
    )

# ========== PLUGIN SDK (Java) ==========

// plugin-sdk/src/main/java/com/aispeaker/sdk/Plugin.java
package com.aispeaker.sdk;

import java.lang.annotation.*;

/**
 * AI Speaker 플러그인 어노테이션
 */
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Plugin {
    /**
     * 플러그인 고유 ID
     */
    String id();
    
    /**
     * 플러그인 이름
     */
    String name();
    
    /**
     * 플러그인 버전
     */
    String version();
    
    /**
     * 지원하는 인텐트 목록
     */
    String[] intents();
    
    /**
     * 플러그인 카테고리
     */
    PluginCategory category() default PluginCategory.UTILITY;
    
    /**
     * 필요한 권한
     */
    String[] permissions() default {};
    
    /**
     * 플러그인 설명
     */
    String description() default "";
    
    /**
     * 작성자
     */
    String author() default "";
}

// plugin-sdk/src/main/java/com/aispeaker/sdk/ConversationPlugin.java
package com.aispeaker.sdk;

import java.util.List;
import java.util.concurrent.CompletableFuture;

/**
 * AI Speaker 대화 플러그인 인터페이스
 */
public interface ConversationPlugin {
    
    /**
     * 플러그인 정보
     */
    PluginInfo getInfo();
    
    /**
     * 지원하는 인텐트 목록
     */
    List<String> getSupportedIntents();
    
    /**
     * 이 플러그인이 해당 인텐트를 처리할 수 있는지 확인
     */
    boolean canHandle(Intent intent, Context context);
    
    /**
     * 플러그인 실행 (동기)
     */
    PluginResponse execute(Intent intent, Context context) throws PluginException;
    
    /**
     * 플러그인 실행 (비동기)
     */
    default CompletableFuture<PluginResponse> executeAsync(Intent intent, Context context) {
        return CompletableFuture.supplyAsync(() -> {
            try {
                return execute(intent, context);
            } catch (PluginException e) {
                throw new RuntimeException(e);
            }
        });
    }
    
    /**
     * 스트리밍 응답 지원
     */
    default boolean supportsStreaming() {
        return false;
    }
    
    /**
     * 스트리밍 실행
     */
    default PluginStreamResponse stream(Intent intent, Context context) throws PluginException {
        throw new UnsupportedOperationException("Streaming not supported");
    }
    
    /**
     * 프로액티브 규칙 목록
     */
    default List<ProactiveRule> getProactiveRules() {
        return List.of();
    }
    
    /**
     * 플러그인 초기화
     */
    default void initialize(PluginContext pluginContext) throws PluginException {
        // 선택적 구현
    }
    
    /**
     * 플러그인 종료
     */
    default void shutdown() {
        // 선택적 구현
    }
    
    /**
     * 우선순위 (높을수록 우선)
     */
    default int getPriority() {
        return 0;
    }
}

// plugin-sdk/src/main/java/com/aispeaker/sdk/BasePlugin.java
package com.aispeaker.sdk;

import lombok.extern.slf4j.Slf4j;
import java.util.*;

/**
 * 플러그인 기본 구현 클래스
 */
@Slf4j
public abstract class BasePlugin implements ConversationPlugin {
    
    protected PluginContext pluginContext;
    protected PluginInfo pluginInfo;
    
    @Override
    public void initialize(PluginContext pluginContext) throws PluginException {
        this.pluginContext = pluginContext;
        
        // 어노테이션에서 정보 추출
        Plugin annotation = this.getClass().getAnnotation(Plugin.class);
        if (annotation != null) {
            this.pluginInfo = PluginInfo.builder()
                .id(annotation.id())
                .name(annotation.name())
                .version(annotation.version())
                .category(annotation.category())
                .description(annotation.description())
                .author(annotation.author())
                .supportedIntents(Arrays.asList(annotation.intents()))
                .requiredPermissions(Arrays.asList(annotation.permissions()))
                .build();
        }
        
        onInitialize();
    }
    
    /**
     * 하위 클래스에서 구현할 초기화 로직
     */
    protected void onInitialize() throws PluginException {
        // 선택적 구현
    }
    
    @Override
    public PluginInfo getInfo() {
        return pluginInfo;
    }
    
    @Override
    public List<String> getSupportedIntents() {
        return pluginInfo != null ? pluginInfo.getSupportedIntents() : List.of();
    }
    
    @Override
    public boolean canHandle(Intent intent, Context context) {
        return getSupportedIntents().contains(intent.getName());
    }
    
    /**
     * 다른 플러그인 호출
     */
    protected <T> CompletableFuture<T> callPlugin(String pluginId, Object request, Class<T> responseType) {
        return pluginContext.getPluginClient(pluginId)
            .call(request, responseType);
    }
    
    /**
     * 이벤트 발행
     */
    protected void publishEvent(PluginEvent event) {
        pluginContext.getEventBus().publish(event);
    }
    
    /**
     * 이벤트 구독
     */
    protected void subscribeEvent(String eventType, PluginEventHandler handler) {
        pluginContext.getEventBus().subscribe(eventType, handler);
    }
    
    /**
     * 외부 API 호출
     */
    protected <T> CompletableFuture<T> callExternalApi(
        String url,
        HttpMethod method,
        Object body,
        Class<T> responseType
    ) {
        return pluginContext.getHttpClient()
            .request(url, method, body, responseType);
    }
    
    /**
     * 설정 값 가져오기
     */
    protected String getConfig(String key) {
        return pluginContext.getConfiguration().getString(key);
    }
    
    /**
     * 설정 값 가져오기 (기본값 포함)
     */
    protected String getConfig(String key, String defaultValue) {
        return pluginContext.getConfiguration().getString(key, defaultValue);
    }
    
    /**
     * 캐시 접근
     */
    protected PluginCache getCache() {
        return pluginContext.getCache();
    }
    
    /**
     * 로깅
     */
    protected void log(String message, Object... args) {
        log.info("[{}] " + message, pluginInfo.getName(), args);
    }
}

// plugin-sdk/src/main/java/com/aispeaker/sdk/PluginResponse.java
package com.aispeaker.sdk;

import lombok.Builder;
import lombok.Data;
import java.util.*;

/**
 * 플러그인 응답
 */
@Data
@Builder
public class PluginResponse {
    /**
     * 음성 응답 텍스트
     */
    private String speech;
    
    /**
     * 화면 표시 텍스트 (선택적)
     */
    private String displayText;
    
    /**
     * 후속 액션 목록
     */
    @Builder.Default
    private List<Action> actions = new ArrayList<>();
    
    /**
     * 컨텍스트 업데이트
     */
    @Builder.Default
    private Map<String, Object> contextUpdate = new HashMap<>();
    
    /**
     * 응답 신뢰도 (0.0 ~ 1.0)
     */
    @Builder.Default
    private double confidence = 1.0;
    
    /**
     * 대화 종료 여부
     */
    @Builder.Default
    private boolean endConversation = false;
    
    /**
     * 추천 응답 (사용자에게 보여줄 버튼 등)
     */
    @Builder.Default
    private List<String> suggestions = new ArrayList<>();
    
    /**
     * 미디어 응답 (이미지, 비디오 등)
     */
    private MediaResponse media;
    
    /**
     * 메타데이터
     */
    @Builder.Default
    private Map<String, Object> metadata = new HashMap<>();
    
    /**
     * 빠른 응답 생성 헬퍼
     */
    public static PluginResponse of(String speech) {
        return PluginResponse.builder()
            .speech(speech)
            .build();
    }
    
    /**
     * 액션 추가
     */
    public PluginResponse addAction(Action action) {
        this.actions.add(action);
        return this;
    }
    
    /**
     * 컨텍스트 업데이트 추가
     */
    public PluginResponse updateContext(String key, Object value) {
        this.contextUpdate.put(key, value);
        return this;
    }
}

// plugin-sdk/src/main/java/com/aispeaker/sdk/examples/WeatherPlugin.java
package com.aispeaker.sdk.examples;

import com.aispeaker.sdk.*;
import lombok.extern.slf4j.Slf4j;
import java.util.concurrent.CompletableFuture;

/**
 * 날씨 플러그인 예제
 */
@Slf4j
@Plugin(
    id = "weather-example",
    name = "날씨 정보",
    version = "1.0.0",
    intents = {"weather.current", "weather.forecast", "weather.clothes"},
    category = PluginCategory.INFORMATION,
    permissions = {"location", "internet"},
    description = "날씨 정보를 제공하고 옷차림을 추천합니다",
    author = "AI Speaker Team"
)
public class WeatherPlugin extends BasePlugin {
    
    private static final String WEATHER_API_URL = "https://api.weather.com/v1";
    private static final String API_KEY_CONFIG = "weather.api.key";
    
    @Override
    protected void onInitialize() throws PluginException {
        // API 키 확인
        String apiKey = getConfig(API_KEY_CONFIG);
        if (apiKey == null || apiKey.isEmpty()) {
            throw new PluginException("Weather API key not configured");
        }
        
        log("Weather plugin initialized with API key: {}****", apiKey.substring(0, 4));
    }
    
    @Override
    public PluginResponse execute(Intent intent, Context context) throws PluginException {
        String location = extractLocation(intent, context);
        
        switch (intent.getName()) {
            case "weather.current":
                return getCurrentWeather(location);
                
            case "weather.forecast":
                return getWeatherForecast(location);
                
            case "weather.clothes":
                return getClothingRecommendation(location);
                
            default:
                throw new PluginException("Unsupported intent: " + intent.getName());
        }
    }
    
    private PluginResponse getCurrentWeather(String location) throws PluginException {
        try {
            // 캐시 확인
            String cacheKey = "weather:current:" + location;
            WeatherData cached = getCache().get(cacheKey, WeatherData.class);
            
            if (cached != null) {
                log("Cache hit for location: {}", location);
                return buildWeatherResponse(cached);
            }
            
            // API 호출
            String apiKey = getConfig(API_KEY_CONFIG);
            String url = WEATHER_API_URL + "/current?location=" + location + "&key=" + apiKey;
            
            CompletableFuture<WeatherData> future = callExternalApi(
                url,
                HttpMethod.GET,
                null,
                WeatherData.class
            );
            
            WeatherData weather = future.get();
            
            // 캐시 저장 (10분)
            getCache().put(cacheKey, weather, 600);
            
            return buildWeatherResponse(weather);
            
        } catch (Exception e) {
            log.error("Failed to get weather for location: {}", location, e);
            throw new PluginException("날씨 정보를 가져올 수 없습니다", e);
        }
    }
    
    private PluginResponse buildWeatherResponse(WeatherData weather) {
        String speech = String.format(
            "현재 %s의 날씨는 %s이고, 온도는 %d도입니다. 습도는 %d%%네요.",
            weather.getLocation(),
            weather.getCondition(),
            weather.getTemperature(),
            weather.getHumidity()
        );
        
        return PluginResponse.builder()
            .speech(speech)
            .displayText(speech)
            .contextUpdate(Map.of(
                "lastWeatherQuery", System.currentTimeMillis(),
                "lastQueriedLocation", weather.getLocation()
            ))
            .suggestions(List.of(
                "내일 날씨는?",
                "뭐 입고 나가면 좋을까?",
                "우산 필요해?"
            ))
            .build();
    }
    
    @Override
    public List<ProactiveRule> getProactiveRules() {
        return List.of(
            ProactiveRule.builder()
                .name("morning_weather_alert")
                .description("아침에 날씨 변화 알림")
                .trigger(TriggerExpression.of("time.hour == 7 && motion.detected"))
                .condition(context -> {
                    // 어제와 날씨가 크게 다른지 확인
                    WeatherData today = getWeatherQuietly(context.getUserLocation());
                    WeatherData yesterday = getYesterdayWeather(context.getUserLocation());
                    
                    return Math.abs(today.getTemperature() - yesterday.getTemperature()) > 5;
                })
                .messageTemplate("좋은 아침이에요! 오늘은 어제보다 ${temp_diff}도 ${temp_direction}. ${recommendation}")
                .priority(Priority.HIGH)
                .build(),
                
            ProactiveRule.builder()
                .name("rain_alert")
                .description("비 예보 시 우산 챙김 알림")
                .trigger(TriggerExpression.of("user.leaving && weather.rain.probability > 0.6"))
                .messageTemplate("잠깐! ${rain_time}에 비가 올 예정이에요. 우산을 챙기는 게 좋겠어요.")
                .priority(Priority.CRITICAL)
                .build()
        );
    }
}
```