```
# ========== MONITORING (Prometheus & Grafana) ==========

# monitoring/prometheus/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'ai-speaker-prod'
        
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093
          
    rule_files:
      - '/etc/prometheus/rules/*.yml'
      
    scrape_configs:
      # Kubernetes 서비스 디스커버리
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https
          
      # AI Speaker 서비스 모니터링
      - job_name: 'ai-speaker-services'
        kubernetes_sd_configs:
        - role: pod
          namespaces:
            names:
            - ai-speaker
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
          
      # Node Exporter
      - job_name: 'node-exporter'
        kubernetes_sd_configs:
        - role: endpoints
        relabel_configs:
        - source_labels: [__meta_kubernetes_endpoints_name]
          action: keep
          regex: node-exporter
          
      # Kafka Exporter
      - job_name: 'kafka'
        static_configs:
        - targets: ['kafka-exporter:9308']
          
      # MongoDB Exporter
      - job_name: 'mongodb'
        static_configs:
        - targets: ['mongodb-exporter:9216']
        
      # Redis Exporter
      - job_name: 'redis'
        static_configs:
        - targets: ['redis-exporter:9121']

---
# monitoring/prometheus/alert-rules.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules
  namespace: monitoring
data:
  ai-speaker-alerts.yml: |
    groups:
    - name: ai-speaker-services
      interval: 30s
      rules:
      
      # Dialog Service 알림
      - alert: DialogServiceDown
        expr: up{job="ai-speaker-services", app="dialog-orchestrator"} == 0
        for: 2m
        labels:
          severity: critical
          service: dialog-orchestrator
        annotations:
          summary: "Dialog Orchestrator 서비스 다운"
          description: "Dialog Orchestrator {{ $labels.instance }}가 2분 이상 응답하지 않습니다."
          
      - alert: DialogServiceHighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="ai-speaker-services", app="dialog-orchestrator"}[5m])) > 1
        for: 5m
        labels:
          severity: warning
          service: dialog-orchestrator
        annotations:
          summary: "Dialog Service 응답 시간 증가"
          description: "Dialog Service의 95% 응답 시간이 1초를 초과합니다. (현재: {{ $value }}s)"
          
      - alert: DialogServiceHighErrorRate
        expr: rate(http_requests_total{job="ai-speaker-services", app="dialog-orchestrator", status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
          service: dialog-orchestrator
        annotations:
          summary: "Dialog Service 오류율 증가"
          description: "Dialog Service의 5xx 오류율이 5%를 초과합니다. (현재: {{ $value }})"
          
      # ML Platform 알림
      - alert: MLModelInferenceFailure
        expr: rate(ml_inference_failures_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          service: ml-platform
        annotations:
          summary: "ML 모델 추론 실패율 증가"
          description: "ML 모델 {{ $labels.model_id }}의 추론 실패율이 10%를 초과합니다."
          
      - alert: MLModelHighLatency
        expr: ml_inference_duration_seconds{quantile="0.95"} > 0.5
        for: 5m
        labels:
          severity: warning
          service: ml-platform
        annotations:
          summary: "ML 모델 추론 지연"
          description: "ML 모델 {{ $labels.model_id }}의 95% 추론 시간이 500ms를 초과합니다."
          
      # 리소스 알림
      - alert: PodMemoryUsageHigh
        expr: container_memory_usage_bytes{namespace="ai-speaker"} / container_spec_memory_limit_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod 메모리 사용량 높음"
          description: "{{ $labels.pod }}의 메모리 사용량이 90%를 초과합니다."
          
      - alert: PodCPUUsageHigh
        expr: rate(container_cpu_usage_seconds_total{namespace="ai-speaker"}[5m]) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod CPU 사용량 높음"
          description: "{{ $labels.pod }}의 CPU 사용량이 90%를 초과합니다."
          
      # Kafka 알림
      - alert: KafkaConsumerLag
        expr: kafka_consumer_lag{topic=~".*-events"} > 1000
        for: 5m
        labels:
          severity: warning
          service: kafka
        annotations:
          summary: "Kafka Consumer Lag 증가"
          description: "{{ $labels.consumer_group }}의 {{ $labels.topic }} 토픽 Lag가 1000을 초과합니다."

---
# monitoring/grafana/dashboards/ai-speaker-overview.json
{
  "dashboard": {
    "title": "AI Speaker Platform Overview",
    "uid": "ai-speaker-overview",
    "tags": ["ai-speaker", "overview"],
    "panels": [
      {
        "title": "Request Rate by Service",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{namespace=\"ai-speaker\"}[5m])) by (app)",
            "legendFormat": "{{ app }}"
          }
        ],
        "gridPos": { "h": 8, "w": 12, "x": 0, "y": 0 }
      },
      {
        "title": "Error Rate by Service",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{namespace=\"ai-speaker\", status=~\"5..\"}[5m])) by (app)",
            "legendFormat": "{{ app }}"
          }
        ],
        "gridPos": { "h": 8, "w": 12, "x": 12, "y": 0 }
      },
      {
        "title": "Response Time P95",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{namespace=\"ai-speaker\"}[5m])) by (app, le))",
            "legendFormat": "{{ app }}"
          }
        ],
        "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 }
      },
      {
        "title": "Active Devices",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(device_status{status=\"online\"})"
          }
        ],
        "gridPos": { "h": 4, "w": 6, "x": 12, "y": 8 }
      },
      {
        "title": "Active Users",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(increase(user_activity_total[24h])) by (user_id)"
          }
        ],
        "gridPos": { "h": 4, "w": 6, "x": 18, "y": 8 }
      },
      {
        "title": "Dialog Processing Time",
        "type": "heatmap",
        "targets": [
          {
            "expr": "sum(rate(dialog_processing_duration_seconds_bucket[5m])) by (le)",
            "format": "heatmap"
          }
        ],
        "gridPos": { "h": 8, "w": 12, "x": 0, "y": 16 }
      },
      {
        "title": "ML Model Performance",
        "type": "table",
        "targets": [
          {
            "expr": "ml_model_metrics{namespace=\"ai-speaker\"}",
            "format": "table"
          }
        ],
        "gridPos": { "h": 8, "w": 12, "x": 12, "y": 16 }
      }
    ]
  }
}

# ========== CI/CD PIPELINE ==========

# .github/workflows/build-and-deploy.yml
name: Build and Deploy

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_PREFIX: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: [api-gateway, user-service, device-registry, dialog-orchestrator, context-service]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up JDK 17
      uses: actions/setup-java@v3
      with:
        java-version: '17'
        distribution: 'temurin'
    
    - name: Cache Gradle packages
      uses: actions/cache@v3
      with:
        path: |
          ~/.gradle/caches
          ~/.gradle/wrapper
        key: ${{ runner.os }}-gradle-${{ hashFiles('**/*.gradle*', '**/gradle-wrapper.properties') }}
        restore-keys: |
          ${{ runner.os }}-gradle-
    
    - name: Run tests
      working-directory: services/${{ matrix.service }}
      run: ./gradlew test
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.service }}
        path: services/${{ matrix.service }}/build/test-results

  build-java-services:
    needs: test
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: [api-gateway, user-service, device-registry, dialog-orchestrator, context-service]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up JDK 17
      uses: actions/setup-java@v3
      with:
        java-version: '17'
        distribution: 'temurin'
    
    - name: Build application
      working-directory: services/${{ matrix.service }}
      run: ./gradlew bootBuildImage --imageName=${{ env.REGISTRY }}/${{ env.IMAGE_PREFIX }}/${{ matrix.service }}:${{ github.sha }}
    
    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Push image
      run: |
        docker push ${{ env.REGISTRY }}/${{ env.IMAGE_PREFIX }}/${{ matrix.service }}:${{ github.sha }}
        docker tag ${{ env.REGISTRY }}/${{ env.IMAGE_PREFIX }}/${{ matrix.service }}:${{ github.sha }} \
          ${{ env.REGISTRY }}/${{ env.IMAGE_PREFIX }}/${{ matrix.service }}:latest
        docker push ${{ env.REGISTRY }}/${{ env.IMAGE_PREFIX }}/${{ matrix.service }}:latest

  build-node-services:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: [automation-engine]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: services/${{ matrix.service }}/package-lock.json
    
    - name: Install dependencies
      working-directory: services/${{ matrix.service }}
      run: npm ci
    
    - name: Run tests
      working-directory: services/${{ matrix.service }}
      run: npm test
    
    - name: Build Docker image
      uses: docker/build-push-action@v4
      with:
        context: services/${{ matrix.service }}
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_PREFIX }}/${{ matrix.service }}:${{ github.sha }}
          ${{ env.REGISTRY }}/${{ env.IMAGE_PREFIX }}/${{ matrix.service }}:latest

  build-python-services:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: [ml-platform]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      working-directory: services/${{ matrix.service }}
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Run tests
      working-directory: services/${{ matrix.service }}
      run: pytest tests/ --cov=app --cov-report=xml
    
    - name: Build Docker image
      uses: docker/build-push-action@v4
      with:
        context: services/${{ matrix.service }}
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_PREFIX }}/${{ matrix.service }}:${{ github.sha }}
          ${{ env.REGISTRY }}/${{ env.IMAGE_PREFIX }}/${{ matrix.service }}:latest

  deploy-staging:
    needs: [build-java-services, build-node-services, build-python-services]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Configure kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
    
    - name: Set up Helm
      uses: azure/setup-helm@v3
      with:
        version: 'v3.13.0'
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ap-northeast-2
    
    - name: Update kubeconfig
      run: aws eks update-kubeconfig --name ai-speaker-staging --region ap-northeast-2
    
    - name: Deploy to staging
      run: |
        helm upgrade --install ai-speaker ./helm-charts/ai-speaker \
          --namespace ai-speaker-staging \
          --create-namespace \
          --values ./helm-charts/ai-speaker/values-staging.yaml \
          --set global.image.tag=${{ github.sha }} \
          --wait

  deploy-production:
    needs: [build-java-services, build-node-services, build-python-services]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Configure kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
    
    - name: Set up Helm
      uses: azure/setup-helm@v3
      with:
        version: 'v3.13.0'
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ap-northeast-2
    
    - name: Update kubeconfig
      run: aws eks update-kubeconfig --name ai-speaker-production --region ap-northeast-2
    
    - name: Deploy to production (Blue/Green)
      run: |
        # 새 버전을 Green으로 배포
        helm upgrade --install ai-speaker-green ./helm-charts/ai-speaker \
          --namespace ai-speaker \
          --values ./helm-charts/ai-speaker/values-production.yaml \
          --set global.image.tag=${{ github.sha }} \
          --set global.deployment.color=green \
          --wait
        
        # 트래픽 점진적 전환
        kubectl apply -f - <<EOF
        apiVersion: networking.istio.io/v1beta1
        kind: VirtualService
        metadata:
          name: ai-speaker-vs
          namespace: ai-speaker
        spec:
          http:
          - match:
            - headers:
                canary:
                  exact: "true"
            route:
            - destination:
                host: ai-speaker
                subset: green
              weight: 100
          - route:
            - destination:
                host: ai-speaker
                subset: blue
              weight: 90
            - destination:
                host: ai-speaker
                subset: green
              weight: 10
        EOF
        
        # 모니터링 및 검증
        sleep 300  # 5분 대기
        
        # 완전 전환
        kubectl patch virtualservice ai-speaker-vs -n ai-speaker --type merge -p '
        {
          "spec": {
            "http": [
              {
                "route": [
                  {
                    "destination": {
                      "host": "ai-speaker",
                      "subset": "green"
                    },
                    "weight": 100
                  }
                ]
              }
            ]
          }
        }'

# ========== HELM CHARTS ==========

# helm-charts/ai-speaker/Chart.yaml
apiVersion: v2
name: ai-speaker
description: AI Speaker IoT Platform Helm chart
type: application
version: 2.0.0
appVersion: "2.0.0"
dependencies:
  - name: postgresql
    version: 12.1.2
    repository: https://charts.bitnami.com/bitnami
    condition: postgresql.enabled
  - name: mongodb
    version: 13.6.2
    repository: https://charts.bitnami.com/bitnami
    condition: mongodb.enabled
  - name: redis
    version: 17.3.14
    repository: https://charts.bitnami.com/bitnami
    condition: redis.enabled
  - name: kafka
    version: 19.1.5
    repository: https://charts.bitnami.com/bitnami
    condition: kafka.enabled

---
# helm-charts/ai-speaker/values.yaml
global:
  image:
    registry: ghcr.io
    repository: ai-speaker
    tag: latest
    pullPolicy: IfNotPresent
  
  deployment:
    replicaCount: 3
    color: blue
  
  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "1Gi"
      cpu: "500m"
  
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 20
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80

# Service-specific configurations
apiGateway:
  enabled: true
  image:
    repository: api-gateway
  service:
    type: LoadBalancer
    port: 80
  ingress:
    enabled: true
    className: nginx
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/rate-limit: "100"
    hosts:
      - host: api.ai-speaker.io
        paths:
          - path: /
            pathType: Prefix
    tls:
      - secretName: api-tls
        hosts:
          - api.ai-speaker.io

userService:
  enabled: true
  image:
    repository: user-service
  replicaCount: 3
  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "1Gi"
      cpu: "500m"

deviceRegistry:
  enabled: true
  image:
    repository: device-registry
  replicaCount: 3

dialogOrchestrator:
  enabled: true
  image:
    repository: dialog-orchestrator
  replicaCount: 5
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"
  websocket:
    enabled: true
    port: 9090

contextService:
  enabled: true
  image:
    repository: context-service
  replicaCount: 3

automationEngine:
  enabled: true
  image:
    repository: automation-engine
  replicaCount: 2
  nodeRed:
    enabled: true
    persistence:
      enabled: true
      size: 10Gi

mlPlatform:
  enabled: true
  image:
    repository: ml-platform
  replicaCount: 3
  gpu:
    enabled: false
    type: "nvidia.com/gpu"
    count: 1
  modelStorage:
    enabled: true
    size: 100Gi

# Infrastructure components
postgresql:
  enabled: true
  auth:
    postgresPassword: "changeme"
    database: "aispeaker"
  primary:
    persistence:
      enabled: true
      size: 20Gi

mongodb:
  enabled: true
  auth:
    enabled: true
    rootPassword: "changeme"
  persistence:
    enabled: true
    size: 50Gi
  replicaSet:
    enabled: true
    replicas:
      secondary: 2

redis:
  enabled: true
  auth:
    enabled: true
    password: "changeme"
  master:
    persistence:
      enabled: true
      size: 10Gi
  replica:
    replicaCount: 2
    persistence:
      enabled: true
      size: 10Gi

kafka:
  enabled: true
  auth:
    enabled: true
    clientProtocol: sasl
  persistence:
    enabled: true
    size: 50Gi
  zookeeper:
    persistence:
      enabled: true
      size: 10Gi

# Monitoring
monitoring:
  enabled: true
  prometheus:
    enabled: true
    retention: 30d
    storage: 100Gi
  grafana:
    enabled: true
    adminPassword: "changeme"
  alertmanager:
    enabled: true

---
# helm-charts/ai-speaker/templates/deployment.yaml
{{- range $service, $config := .Values }}
{{- if and (kindIs "map" $config) $config.enabled (ne $service "global") (ne $service "monitoring") }}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ $service | kebabcase }}
  labels:
    app: {{ $service | kebabcase }}
    version: {{ $.Values.global.deployment.color }}
    chart: {{ $.Chart.Name }}-{{ $.Chart.Version }}
spec:
  replicas: {{ $config.replicaCount | default $.Values.global.deployment.replicaCount }}
  selector:
    matchLabels:
      app: {{ $service | kebabcase }}
  template:
    metadata:
      labels:
        app: {{ $service | kebabcase }}
        version: {{ $.Values.global.deployment.color }}
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: {{ $service | kebabcase }}
        image: "{{ $.Values.global.image.registry }}/{{ $.Values.global.image.repository }}/{{ $config.image.repository }}:{{ $.Values.global.image.tag }}"
        imagePullPolicy: {{ $.Values.global.image.pullPolicy }}
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        {{- if $config.websocket }}
        - name: websocket
          containerPort: {{ $config.websocket.port }}
          protocol: TCP
        {{- end }}
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "k8s"
        - name: SERVICE_NAME
          value: {{ $service | kebabcase }}
        envFrom:
        - configMapRef:
            name: common-config
        - secretRef:
            name: {{ $service | kebabcase }}-secrets
            optional: true
        resources:
          {{- toYaml ($config.resources | default $.Values.global.resources) | nindent 10 }}
        livenessProbe:
          httpGet:
            path: /actuator/health/liveness
            port: http
          initialDelaySeconds: 60
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: http
          initialDelaySeconds: 30
          periodSeconds: 5
        {{- if $config.gpu }}
        {{- if $config.gpu.enabled }}
        resources:
          limits:
            {{ $config.gpu.type }}: {{ $config.gpu.count }}
        {{- end }}
        {{- end }}
{{- end }}
{{- end }}

---
# helm-charts/ai-speaker/templates/service.yaml
{{- range $service, $config := .Values }}
{{- if and (kindIs "map" $config) $config.enabled (ne $service "global") (ne $service "monitoring") }}
---
apiVersion: v1
kind: Service
metadata:
  name: {{ $service | kebabcase }}
  labels:
    app: {{ $service | kebabcase }}
spec:
  type: {{ $config.service.type | default "ClusterIP" }}
  ports:
  - port: {{ $config.service.port | default 80 }}
    targetPort: http
    protocol: TCP
    name: http
  {{- if $config.websocket }}
  - port: {{ $config.websocket.port }}
    targetPort: websocket
    protocol: TCP
    name: websocket
  {{- end }}
  selector:
    app: {{ $service | kebabcase }}
{{- end }}
{{- end }}

# ========== DOCKER FILES ==========

# services/dialog-orchestrator/Dockerfile
FROM eclipse-temurin:17-jre-alpine AS runtime

RUN apk add --no-cache curl

WORKDIR /app

COPY build/libs/*-SNAPSHOT.jar app.jar

# JVM 최적화 옵션
ENV JAVA_OPTS="-XX:+UseContainerSupport \
    -XX:MaxRAMPercentage=75.0 \
    -XX:+UseG1GC \
    -XX:+OptimizeStringConcat \
    -Djava.security.egd=file:/dev/./urandom"

EXPOSE 8080 9090

HEALTHCHECK --interval=30s --timeout=3s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8080/actuator/health || exit 1

ENTRYPOINT ["sh", "-c", "java $JAVA_OPTS -jar app.jar"]

---
# services/ml-platform/Dockerfile
FROM python:3.10-slim AS builder

WORKDIR /app

# 시스템 의존성 설치
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Python 의존성 설치
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

FROM python:3.10-slim AS runtime

WORKDIR /app

# 런타임 의존성만 설치
RUN apt-get update && apt-get install -y \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# 빌더에서 Python 패키지 복사
COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# 애플리케이션 코드 복사
COPY . .

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=3s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]

---
# services/automation-engine/Dockerfile
FROM node:18-alpine AS builder

WORKDIR /app

# 의존성 설치
COPY package*.json ./
RUN npm ci --only=production

FROM node:18-alpine AS runtime

WORKDIR /app

# 런타임 의존성
RUN apk add --no-cache curl

# 빌더에서 node_modules 복사
COPY --from=builder /app/node_modules ./node_modules

# 애플리케이션 코드 복사
COPY . .

# Node-RED 데이터 디렉토리
RUN mkdir -p /data && chown -R node:node /data
VOLUME ["/data"]

USER node

EXPOSE 3000 1880

HEALTHCHECK --interval=30s --timeout=3s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:3000/health || exit 1

CMD ["node", "src/app.js"]
```